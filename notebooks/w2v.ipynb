{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tqdm\n",
    "import pymorphy2\n",
    "import logging\n",
    "import os\n",
    "from string import punctuation\n",
    "from nltk import TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "from sqlalchemy import create_engine\n",
    "from src.config import conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pymorphy2.opencorpora_dict.wrapper:Loading dictionaries from /Users/lev4/PycharmProjects/app-sberjobs/sberjobs-trainer/venv/lib/python3.8/site-packages/pymorphy2_dicts_ru/data\n",
      "INFO:pymorphy2.opencorpora_dict.wrapper:format: 2.4, revision: 417127, updated: 2020-10-11T15:05:51.070345\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "logging.basicConfig(level=\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(conn_string):\n",
    "    \"\"\"\n",
    "    Подключается к БД и выкачивает вакансии\n",
    "    \"\"\"\n",
    "    logging.info(\"Подгружаю данные из базы\")\n",
    "    engine = create_engine(conn_string)\n",
    "\n",
    "    df = pd.read_sql_table('vacancy', engine)\n",
    "    logging.info(df.head)\n",
    "    lines = df.vacdescription.tolist()\n",
    "    vacids = df.vacid.tolist()\n",
    "    return lines, vacids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_pipe(lines):\n",
    "    logging.info(\"Готовлю корпус\")\n",
    "    ru_stop_words = stopwords.words('russian')\n",
    "    lines_tok = [TreebankWordTokenizer().tokenize(x) for x in lines]\n",
    "    lines_tok = [[x for x in el if x not in punctuation] for el in lines_tok]\n",
    "    u_norm = [[morph.parse(x)[0][2] for x in el] for el in tqdm(lines_tok)]\n",
    "    u_norm = [[x for x in el if x not in ru_stop_words] for el in tqdm(u_norm)]\n",
    "    corpus = [' '.join(x) for x in u_norm]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l2_norm(x):\n",
    "    return np.sqrt(np.sum(x ** 2))\n",
    "\n",
    "\n",
    "def div_norm(x):\n",
    "    norm_value = l2_norm(x)\n",
    "    if norm_value > 0:\n",
    "        return x * (1.0 / norm_value)\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vacancy_vectors(vacids, corpus):\n",
    "    \"\"\"\n",
    "    Получает вектора профилей пользователей из фасттекста\n",
    "    \"\"\"\n",
    "\n",
    "    from gensim.models import FastText\n",
    "    \n",
    "    vacancy_vectors = {}\n",
    "    logging.info(\"Подгружаем обученную модель FastText\")\n",
    "    fasttext_pth = os.path.join('..','wvmodel','cc.ru.300.bin')\n",
    "    fast_text = FastText.load_fasttext_format(fasttext_pth).wv\n",
    "    \n",
    "    logging.info(\"Собираем векторы предложений\")\n",
    "    for x in tqdm((vacids, corpus)):\n",
    "\n",
    "        text = x[1].split()\n",
    "        text.append('\\n')\n",
    "        matrix = np.zeros((300,), dtype = 'float32')\n",
    "\n",
    "        for word in text:\n",
    "            matrix += div_norm(fast_text.word_vec(word))\n",
    "\n",
    "        vacancy_vectors[x[0]] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_similarities(target_user, candidates):\n",
    "    \"\"\"Получает косинусные сходства сотрудника и кандидатов\"\"\"\n",
    "\n",
    "    tu_sims = {}\n",
    "    for vacid in vacids:\n",
    "        tu_sims[candidates] = cosine_similarity(\n",
    "            vacancy_vectors[vacid],\n",
    "            user_vectors\n",
    "        )[0][0]\n",
    "\n",
    "    return tu_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conn_string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-93a6fd1e3e69>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mlines\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvacids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_lines\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn_string\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'conn_string' is not defined"
     ]
    }
   ],
   "source": [
    "lines, vacids = get_lines(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lines' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-6980abacff20>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtxt_pipe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlines\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'lines' is not defined"
     ]
    }
   ],
   "source": [
    "corpus = txt_pipe(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join('..','data','corpus.pkl'), 'wb') as f:\n",
    "#     pickle.dump(corpus, f)\n",
    "\n",
    "# with open(os.path.join('..','data','vacids.pkl'), 'wb') as f:\n",
    "#     pickle.dump(vacids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/vacids.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-14-92eb2f2afadb>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'..'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'data'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'vacids.pkl'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m     \u001B[0mvacids\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'..'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'data'\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m'corpus.pkl'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mcorpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpickle\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../data/vacids.pkl'"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('..','data','vacids.pkl'), 'rb') as f:\n",
    "    vacids = pickle.load(f)\n",
    "\n",
    "with open(os.path.join('..','data','corpus.pkl'), 'rb') as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vvect = get_vacancy_vectors(vacids, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging.info(\"Подгружаем обученную модель FastText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.models._fasttext_bin:loading 977837 words for fastText model from ../wvmodel/ft_native_300_ru_wiki_lenta_lemmatize.bin\n",
      "INFO:gensim.models.word2vec:resetting layer weights\n",
      "INFO:gensim.models.word2vec:Updating model with new vocabulary\n",
      "INFO:gensim.models.word2vec:New added 977837 unique words (50% of original 1955674) and increased the count of 977837 pre-existing words (50% of original 1955674)\n",
      "INFO:gensim.models.word2vec:deleting the raw counts dictionary of 977837 items\n",
      "INFO:gensim.models.word2vec:sample=0.0001 downsamples 788 most-common words\n",
      "INFO:gensim.models.word2vec:downsampling leaves estimated 830512760 word corpus (120.7% of prior 688220651)\n",
      "INFO:gensim.models.fasttext:loaded (2977837, 300) weight matrix for fastText model from ../wvmodel/ft_native_300_ru_wiki_lenta_lemmatize.bin\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import fasttext\n",
    "fasttext_pth = os.path.join('..','wvmodel','ft_native_300_ru_wiki_lenta_lemmatize.bin')\n",
    "fast_text = fasttext.load_facebook_vectors(fasttext_pth)\n",
    "\n",
    "# with open(os.path.join('..','data','fast_text.pkl'), 'rb') as f:\n",
    "#     fast_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with open(os.path.join('..','data','fast_text.pkl'), 'wb') as f:\n",
    "#     pickle.dump(fast_text, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancy_vectors = {}\n",
    "for x in tqdm(list(zip(vacids, corpus))):\n",
    "    text = x[1].split()\n",
    "    text.append('\\n')\n",
    "    matrix = np.zeros((300,), dtype = 'float32')\n",
    "    for word in text:\n",
    "        matrix += div_norm(fast_text.word_vec(word))\n",
    "    vacancy_vectors[x[0]] = matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"python git data science machine learning\"\n",
    "text = \"сми репутация сторителлинг фактчекинг пресс-релиз коммуникация pr журналист москва\"\n",
    "text = text.split()\n",
    "text.append('\\n')\n",
    "matrix = np.zeros((300,), dtype = 'float32')\n",
    "for word in text:\n",
    "    matrix += div_norm(fast_text.word_vec(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tu_sims = {}\n",
    "for vacid in tqdm(vacids):\n",
    "    tu_sims[vacid] = cosine_similarity(vacancy_vectors[vacid].reshape(1,-1),\n",
    "                                       matrix.reshape(1,-1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tu_sorted = sorted(tu_sims.items(), key=lambda x:x[1], reverse=True)\n",
    "tu_sorted = [x[0] for x in tu_sorted]\n",
    "df = pd.DataFrame({'description':lines, 'vacid':vacids})\n",
    "df = df.set_index('vacid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[tu_sorted[]].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}